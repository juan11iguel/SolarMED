from dataclasses import asdict, fields, is_dataclass
from typing import get_origin, get_args, Union, Any, Type, Literal
from datetime import datetime, timezone
from enum import Enum
import json
import math
from collections.abc import Iterable
import time
import warnings
from loguru import logger
import numpy as np
import pandas as pd
from solarmed_modeling.fsms import MedState
from solarmed_modeling.fsms.med import FsmInputs as MedFsmInputs
from solarmed_optimization import (DecisionVariables, 
                                   DecisionVariablesUpdates, 
                                   dump_at_index_dec_vars,
                                   ProblemSamples,
                                   ProblemParameters,
                                   RealLogicalDecVarDependence,
                                   RealDecVarsBoxBounds,
                                   MedMode,
                                   med_fsm_inputs_table)
# from solarmed_optimization.problems import BaseMinlpProblem # circular import

def deprecated_function():
    warnings.warn("This function is deprecated and will be removed in future versions.", DeprecationWarning)

def fitness_logger(func: callable) -> callable:
    def wrapper(*args, **kwargs) -> Any:
        print(f"Running function {func.__name__} with decision vector: {args[1]}")
        return func(*args, **kwargs)
    return wrapper

def timer_decorator(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()

        initial_states = kwargs.get('initial_states', None)
        max_step_idx = kwargs.get('max_step_idx', None)

        logger.info(f"Function {func.__name__} took {(end_time - start_time):.2f} seconds to run. Evaluated initial states {initial_states} for {max_step_idx} steps")
        return result
    return wrapper

def get_nested_attr(d: dict, attr: str) -> Any:
    """ Get nested attributes separated by a dot in a dictionary """
    keys = attr.split('.')
    for key in keys:
        d = d.get(key, None)
    return d

def flatten_list(nested_list: list[list]) -> list:
    if not isinstance(nested_list[0], Iterable):
        return nested_list # Not actually a nested list
    return [item for sublist in nested_list for item in sublist]
    
def infer_attribute_name(instance: Any, target_type: type) -> str | None:
    """Infers the attribute name in a dataclass based on the type of its value."""
    for field in fields(instance):
        field_type = field.type
        # Check if the field's type matches the target type
        if target_type == field_type or (hasattr(field_type, "__args__") and target_type in field_type.__args__):
            return field.name
    return None

def extract_prefix(text: str) -> str:
    return "_".join(text.split("_pop")[0].split("_"))

class CustomEncoder(json.JSONEncoder):
    """ Custom JSON encoder supporting NumPy arrays and Enums
        Example usage: json.dumps(array, cls=NumpyEncoder)
    """
    def default(self, obj):
        if isinstance(obj, np.ndarray):  # Handle NumPy arrays
            return obj.tolist()
        elif isinstance(obj, Enum):  # Handle Enums
            return obj.value
        return super().default(obj)

def resolve_dataclass_type(fld_type: Type) -> Type | None:
    """Extract the actual dataclass type from optional/union/wrapped types."""
    origin = get_origin(fld_type)
    args = get_args(fld_type)

    if origin is Union:
        # Filter out NoneType (for Optional[...] support)
        for arg in args:
            if isinstance(arg, type) and is_dataclass(arg):
                return arg
    elif isinstance(fld_type, type) and is_dataclass(fld_type):
        return fld_type
    return None
    
def get_valid_modes(fsm_inputs: MedFsmInputs, 
                    lookup_table: dict[tuple[MedMode, MedState], MedFsmInputs] = med_fsm_inputs_table,
                    return_values: bool = True) -> list[MedMode]:
    """ Return valid modes given a set of fsm inputs that can be applied to the system (generated by studying the FSM possible evolutions) """
    valid_modes = [
        key[0] for key, value in lookup_table.items() if value == fsm_inputs
    ]
    if not return_values:
        return valid_modes
    else:
        return [valid_mode.value for valid_mode in valid_modes]

    
def forward_fill_resample(source_array: np.ndarray, target_size: int, dtype: Type = None) -> np.ndarray:
    """Resample a smaller array to a larger array by forward filling

        Args:
            source_array (np.ndarray): Original array
            target_size (int): Size of the target array

        Returns:
            np.ndarray: Resampled array
            
        Example usage
            source_array = np.array([1, 2, 3])
            target_size = 10
            resampled_array = resample_array(source_array, target_size)
            assert len(resampled_array) == target_size, "Resampled array size mismatch"
    """
    if len(source_array) > target_size:
        logger.warning(f"Source array size {len(source_array)} is greater than to target size {target_size}. Returning trimmed source array")
        return source_array[:target_size]
    if len(source_array) == target_size:
        return source_array
    
    dtype: Type = type(source_array[0]) if dtype is None else dtype
    
    span = target_size // len(source_array)  # Integer division
    remainder = target_size % len(source_array)  # Remaining slots to fill
    
    # Repeat each element by span times
    resampled_array = np.repeat(source_array, span)
    
    # Forward fill the remaining slots if target_size isn't a multiple of source_array length
    if remainder > 0:
        resampled_array = np.concatenate((resampled_array, np.repeat(source_array[-1], remainder)))
    
    return resampled_array.astype(dtype)


def downsample_by_segments(source_array:  np.ndarray, target_size: int, dtype: Type = None) -> np.ndarray:
    """
    Downsamples the source array to the target size by selecting the mean value 
    in each segment of the array.

    Parameters:
        source_array (np.ndarray): The input array to be downsampled.
        target_size (int): The desired size of the output array.

    Returns:
        np.ndarray: The downsampled array, where each element is the mean value 
        of a segment from the input array.
    
    Raises:
        ValueError: If target_size is less than 1 or greater than the size of source_array.
        
    Example usage
        source_array = np.array([1, 1000, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9])
        target_size = 5
        downsampled_array = downsample_by_segments(source_array, target_size)

        print("Original array:", source_array)
        print("Downsampled array:", downsampled_array)
        assert len(downsampled_array) == target_size, "Downsampled array size mismatch"
    """
    if target_size < 1 or target_size > len(source_array):
        raise ValueError("target_size must be between 1 and the size of the source array.")

    dtype = dtype or source_array[0].dtype

    segment_size = len(source_array) / target_size
    return np.array([
        source_array[int(i * segment_size):int((i + 1) * segment_size)].mean()
        for i in range(target_size)
    ]).astype(dtype)
    

def resample_timeseries_range_and_fill(series: pd.Series, sample_time: int, dt_span: tuple[datetime, datetime] = None) -> pd.Series:
    """Extracts a given range from a larger time series and resamples it to a given sample time,
    finally fills the last value

    Args:
        series (pd.Series): Series to be resampled
        sample_time (int): Sample time in seconds
        dt_span (tuple[datetime, datetime]): Start and end datetime of the range to be extracted

    Returns:
        pd.Series: Resampled series with the last value filled
    """
    series = series.copy().loc[dt_span[0]:dt_span[1]] if dt_span is not None else series.copy()
    series_ = series.resample(f'{sample_time}s', origin="start").mean()
    
    return pd.concat([
        series_, 
        pd.Series([series.loc[series_.index[-1]:dt_span[1]].mean()], index=[dt_span[1]])
    ])
    

def resample_decision_variables(dec_vars: DecisionVariables, dec_var_updates: DecisionVariablesUpdates) -> DecisionVariables:
    """Resample decision variable object

    Args:
        dec_vars (DecisionVariables): _description_
        dec_var_updates (DecisionVariablesUpdates): _description_

    Returns:
        _type_: _description_
    """
    
    for var_id, var_values in asdict(dec_vars).items():
        target_size = getattr(dec_var_updates, var_id)
        if target_size >= len(var_values):
            setattr(dec_vars, var_id, forward_fill_resample(var_values, target_size))
        elif target_size < len(var_values):
            setattr(dec_vars, var_id, downsample_by_segments(var_values, target_size))
            
    return dec_vars
    

def decision_vector_to_decision_variables(x: np.ndarray, dec_var_updates: DecisionVariablesUpdates,
                                          span: Literal["optim_window", "optim_step", "none"],
                                          sample_time_mod: int = None, 
                                          optim_window_time: int = None,
                                          sample_time_opt: int = None) -> DecisionVariables:
    """ From decision vector x to DecisionVariables instance
        The decision vector is the one defined in the problem class:
        x = [ [1,...,n_updates_x1], [1,...,n_updates_x2], ..., [1,...,n_updates_xNdec_vars] ]
        where n_updates_xi is the number of updates for decision variable i along the prediction horizon
        
        The resulting instance contains a vector for every decision variable with
        a sample time equal to the one of the model.
    
        - When span is set to optim window, for every decision variable, all its
        updates are included in the resulting Decision Variables instance and the
        number of elements is equal to the number of model evaluations along the 
        optimization window.
        
        - When span is set to optim step, for every decision variable, only the updates
        contained within one optimization step are included in the resulting instance, 
        and the number of elements is equal to the number of model evaluations along 
        one optimization step.
    """
    
    assert span in ["optim_window", "optim_step", "none"], "span should be either 'optim_window' or 'optim_step'"
    if span in ["optim_window", "optim_step"]:
        assert sample_time_mod is not None, "If span is 'optim_window' or 'optim_step', sample_time_mod should be provided"
        assert optim_window_time is not None, "If span is 'optim_window', optim_window_time should be provided"
    if span == "optim_step":
        assert sample_time_opt is not None, "If span is 'optim_step', sample_time_opt should be provided"

    n_evals_mod = None    
    if span == "optim_step":
        # As many model evaluations as samples that fit in the optimization step 
        n_evals_mod = math.floor(sample_time_opt / sample_time_mod)
    elif span == "optim_window":
        # As many model evaluations as samples that fit in the optimization window
        n_evals_mod = math.floor(optim_window_time / sample_time_mod)
        
    # Build the decision variables dictionary in which every variable is "resampled" to the model sample time
    decision_dict: dict[str, np.ndarray] = {}
    cnt = 0
    # for var_id, num_updates_optim_window in asdict(dec_var_updates).items():
    # dec_var_updates does not necessary follow the same order as DecisionVariables
    for fld in fields(DecisionVariables):
        var_id = fld.name
        num_updates_optim_window = getattr(dec_var_updates, var_id)
        
        if span == "optim_step":
            # Only the updates in the optimization step are considered
            # |o----o----o----o--| : 4 updates in the optimization window (size = optim_window_time)
            # |________| : 2 updates in the optimization step (size = sample_time_opt)
            num_updates = math.floor(num_updates_optim_window * sample_time_opt / optim_window_time)
            # print(f"{num_updates=}")
        else:
            # All the updates in the optimization window are considered
            num_updates = num_updates_optim_window

        if n_evals_mod is not None:            
            decision_dict[var_id] = forward_fill_resample(x[cnt:cnt+num_updates], target_size=n_evals_mod)
        else:
            decision_dict[var_id] = x[cnt:cnt+num_updates]
        cnt += num_updates_optim_window
    
    return DecisionVariables(**decision_dict)

def decision_variables_to_decision_vector(dec_vars: DecisionVariables, dec_var_updates: DecisionVariablesUpdates = None) -> np.ndarray:
    """ From DecisionVariables instance to decision vector x
        The decision vector is the one defined in the problem class:
        x = [ [1,...,n_updates_x1], [1,...,n_updates_x2], ..., [1,...,n_updates_xNdec_vars] ]
        where n_updates_xi is the number of updates for decision variable i along the prediction horizon
    """
    if dec_var_updates is not None:
        dec_vars = resample_decision_variables(dec_vars, dec_var_updates)
                
    return np.concatenate([var_values for var_values in asdict(dec_vars).values()])
        
def compute_dec_var_differences(dec_vars: dict[str, float], model_dec_vars: dict[str, float], model_dec_var_ids: list[str]) -> np.ndarray[float]:
    """ This implementation compared to initializing two dataframes is much faster:
    # Results
    {
        'list_comprehension_time': 0.00038123130798339844,
        'dataframe_time': 0.0037851333618164062,
    }
    """
    # Align both dictionaries' values using the common order of model_dec_var_ids
    # ordered_dv_values = [dv_dict[key] for key in model_dec_var_ids] # Already ordered
    dec_vars_values: list[float] = list(dec_vars.values())
    ordered_model_values: list[float] = [model_dec_vars[key] for key in model_dec_var_ids]

    # Compute absolute differences
    return np.abs(
        np.array(dec_vars_values, dtype=float) - np.array(ordered_model_values, dtype=float)
    )
    
def validate_dec_var_updates(dec_var_updates: DecisionVariablesUpdates, optim_window_time: int, sample_time_mod: int) -> None:
    """ 
    Validate that the number of updates for each decision variable is within the bounds
    of the optimization window or optimization step.
    """
    n_evals_mod_in_hor_window: int = math.floor(optim_window_time  / sample_time_mod)
    max_dec_var_updates: int = n_evals_mod_in_hor_window
    min_dec_var_updates: int = 1

    for var_id, value in asdict(dec_var_updates).items():
        try:
            assert value <= max_dec_var_updates and value >= min_dec_var_updates, \
            f"Invalid number of updates for variable {var_id}, n updates = {value}, should be {min_dec_var_updates} <= n updates <= {max_dec_var_updates}"
        except AssertionError as e:
            logger.warning(f"{e}. Setting value to bound")
            
            if value < min_dec_var_updates:
                setattr(dec_var_updates, var_id, min_dec_var_updates)
            else:
                setattr(dec_var_updates, var_id, max_dec_var_updates)
            

def add_bounds_to_dataframe(df: pd.DataFrame, problem, target: Literal['optim_step', 'optim_window'], df_idx: int = 0) -> pd.DataFrame:
                            #target_size: int, source_size: int = None, 
    
    for var_idx, var_id in enumerate(problem.dec_var_ids):
        if f"lower_bounds_{var_id}" not in df.columns:
            df[f"upper_bounds_{var_id}"] = np.nan
            df[f"lower_bounds_{var_id}"] = np.nan
                
        if target == "optim_step":
            source_size = problem.n_updates_per_opt_step[var_idx]
            target_size = problem.n_evals_mod_in_opt_step
        else:
            source_size = getattr(problem.dec_var_updates, var_id)
            target_size = problem.n_evals_mod_in_hor_window
                
        if var_id in problem.dec_var_int_ids:
            discrete_bounds = problem.integer_dec_vars_mapping[var_id][:source_size]
            upper_bounds_eval = [np.max(db) for db in discrete_bounds]
            lower_bounds_eval = [np.min(db) for db in discrete_bounds]
        else:
            upper_bounds_eval = problem.box_bounds_upper[var_idx][:source_size]
            lower_bounds_eval = problem.box_bounds_lower[var_idx][:source_size]
        
        lower_bounds_eval = forward_fill_resample(lower_bounds_eval, target_size=target_size)
        upper_bounds_eval = forward_fill_resample(upper_bounds_eval, target_size=target_size)
        
        df.loc[df_idx:df_idx+target_size, f"upper_bounds_{var_id}"] = upper_bounds_eval
        df.loc[df_idx:df_idx+target_size, f"lower_bounds_{var_id}"] = lower_bounds_eval
        
    return df

def add_dec_vars_to_dataframe(df: pd.DataFrame, dec_vars: DecisionVariables, df_idx: int = 0) -> pd.DataFrame:
    # UPDATE: This functionality was moved as a method integrated in the DecisionVariables object itself
    warnings.warn("Deprecated function, use DecisionVariables.add_dec_vars_to_dataframe instead")
    
    for var_id, var_values in asdict(dec_vars).items():
        if isinstance(var_values, pd.Series):
            # Ideally var_values should already be compatible with dataframe, not having to trim it here with [0:len(df)]
            df.loc[var_values.index[0:len(df)], f"dec_var_{var_id}"] = var_values.values[0:len(df)]
        else:
            df.loc[df_idx:df_idx+len(var_values), f"dec_var_{var_id}"] = var_values
        
    return df

def times_to_samples(pp: ProblemParameters) -> ProblemSamples:
    # Times to samples transformation
    
    assert pp.episode_duration is not None, "Episode duration must be defined before calling this function"
    
    n_evals_mod_in_hor_window = math.floor(pp.optim_window_time  / pp.sample_time_mod)
    max_dec_var_updates = n_evals_mod_in_hor_window
    episode_samples = pp.episode_duration // pp.sample_time_mod
    optim_window_samples = pp.optim_window_time // pp.sample_time_mod
    n_evals_mod_in_opt_step = math.floor(pp.sample_time_opt / pp.sample_time_mod)
    span = math.ceil(pp.fixed_model_params.sf.delay_span / pp.sample_time_mod)
    idx_start = span if pp.idx_start is None else pp.idx_start
    
    return ProblemSamples(
        n_evals_mod_in_hor_window = n_evals_mod_in_hor_window,
        n_evals_mod_in_opt_step = n_evals_mod_in_opt_step,
        max_dec_var_updates = max_dec_var_updates,
        episode_samples = episode_samples,
        optim_window_samples = optim_window_samples,
        max_opt_steps = (episode_samples-idx_start-optim_window_samples) // n_evals_mod_in_opt_step - 1,
        default_n_dec_var_updates = min(math.floor(pp.optim_window_time / pp.sample_time_opt), max_dec_var_updates),
        span=span
    )

def validate_real_dec_vars(dec_vars: DecisionVariables, real_dec_var_bounds: RealDecVarsBoxBounds) -> DecisionVariables:
    """
    Validate real decision variables values. 
    - If a value is below the lower limit, set it to the lower limit
    - If a value is above the upper limit, set it to the upper limit
    """
    for var_id, bounds in asdict(real_dec_var_bounds).items():
        lower_limit, upper_limit = bounds
        var_values = getattr(dec_vars, var_id)
        integer_dec_var_val = forward_fill_resample(getattr( dec_vars, RealLogicalDecVarDependence[var_id].value), 
                                                    target_size=var_values.shape[0])
        
        var_values = np.clip(var_values, lower_limit, upper_limit)
        var_values = var_values * integer_dec_var_val
        
        setattr(dec_vars, var_id, var_values)
    
    return dec_vars

def find_n_best_values_in_list(source_list: list[list[float]], n: int, objective: Literal["minimize", "maximize"] = "minimize") -> tuple[list[int], list[float]]:

    best_idxs = [None] * n
    best_fitness_list = [float("inf")] * n
    
    if objective == "minimize":
        fitness_list = [np.min(np.array(case)) for case in source_list if len(case) > 0]
    else:
        fitness_list = [np.max(np.array(case)) for case in source_list if len(case) > 0]

    for idx, fitness in enumerate(fitness_list):            
        for i, best_fitness in enumerate(best_fitness_list):
            # print(f"{fitness=} vs {best_fitness=} in position {i}")

            if (objective == "minimize" and fitness < best_fitness) or (objective == "maximize" and fitness > best_fitness):
                # Shift elements to the right from i
                best_fitness_list[i+1:] = best_fitness_list[i:-1]
                best_idxs[i+1:] = best_idxs[i:-1]
                
                # Insert new best fitness and index at position i
                best_fitness_list[i] = fitness
                best_idxs[i] = idx
                break

    logger.info(f"{best_fitness_list=} at {best_idxs=}")
    return best_idxs, best_fitness_list

def get_start_and_end_datetimes(series: pd.Series | list[pd.Series]) -> tuple[datetime | None, datetime | None]:
    """
    Determine the first active timestamp (start) and the first inactive timestamp after the last active (end).
    If multiple series are provided, the start is the earliest global active, and the end is the latest global inactive.
    """
    if not isinstance(series, list):
        series = [series]

    start_dt = None
    end_dt = None

    for ser in series:
        # Identify indices of active and inactive states
        active_indices = ser[ser > 0].index
        inactive_indices = ser[ser <= 0].index

        # Determine start: Earliest active across all series
        if not active_indices.empty:
            first_active_idx = active_indices[0]
            start_dt = min(start_dt, first_active_idx) if start_dt else first_active_idx

        # Determine end: First inactive after the last active across all series
        if not active_indices.empty:
            last_active_idx = active_indices[-1]

            # Find the first inactive after the last active
            post_last_active = inactive_indices[inactive_indices > last_active_idx]
            if not post_last_active.empty:
                first_inactive_after_last_active = post_last_active[0]
                end_dt = max(end_dt, first_inactive_after_last_active) if end_dt else first_inactive_after_last_active

    # If no start was found, set it to the first index of the first series
    if start_dt is None:
        start_dt = series[0].index[0]

    # If no end was determined, set it to the first index of the first series
    if end_dt is None:
        end_dt = start_dt

    return start_dt, end_dt

def select_best_alternative(df: pd.DataFrame, std_penalty_weight: float =1.0, worst_case_penalty_weight: float =1.0) -> tuple[int, pd.DataFrame]:
    """
    Select the best alternative based on average performance, consistency (std dev), and worst-case scenario.
    
    Parameters:
    - df: pd.DataFrame
        Rows = alternatives, Columns = scenarios
    - std_penalty_weight: float
        How much to penalize alternatives with high standard deviation
    - worst_case_penalty_weight: float
        How much to penalize based on the worst-case performance
    
    Returns:
    - best_alternative: str
        Index label of the best alternative
    """
    # Calculate metrics
    means = df.mean(axis=1)
    if len(df.columns) > 1:
        stds = df.std(axis=1)
    else:
        stds = pd.Series([0] * len(df.index), index=df.index)
    worst_cases = df.max(axis=1)
    
    # Composite score (lower is better)
    scores = means + std_penalty_weight * stds + worst_case_penalty_weight * worst_cases
    
    # Reporting
    report = pd.DataFrame({
        'Mean': means,
        'Std Dev': stds,
        'Worst Case': worst_cases,
        'Composite Score': scores
    }).sort_values('Composite Score')
    
    print("\n=== Alternative Evaluation Report ===\n")
    print(report)
    print("\n======================================\n")
    
    # Pick the best
    best_alternative = scores.idxmin()
    print(f"Selected Best Alternative: {best_alternative}")
    
    return best_alternative, report


def condition_result_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Condition the optimization results DataFrame to clean NaN values and some more."""

    df["med_active"] = df["med_active"].astype(bool).fillna(False)
    df["sf_active"] = df["sf_active"].astype(bool).fillna(False)
    df["ts_active"] = df["ts_active"].astype(bool).fillna(False)
    df["sf_ts_state"] = df["sf_ts_state"].astype(float).fillna(0).astype(int)
    df["med_state"] = df["med_state"].astype(float).fillna(0).astype(int)
    df["Pth_hx_p"] = df["Pth_hx_p"].fillna(0.0)
    df["Pth_hx_s"] = df["Pth_hx_s"].fillna(0.0)
    df["Pth_ts_dis"] = df["Pth_ts_dis"].fillna(0.0)
    df["net_profit"] = df["net_profit"].fillna(0.0)
    df["net_loss"] = df["net_loss"].fillna(0.0)
    df["Jtotal"] = df["Jtotal"].fillna(0.0)
    
    for var_id in df.columns:
        if not var_id.startswith("dec_var_"):
            continue
        df[var_id] = df[var_id].astype(float).fillna(0.0)

    # Infer objects to avoid silent downcasting
    # df = df.infer_objects(copy=False)

    # New plot variables
    df["cumulative_net_profit"] = df["net_profit"].cumsum()
    # df["sfts_mode"] = df["sf_ts_state"] + 1
    # df["med_mode"] = df["med_state"].apply(lambda x: 2 if 1 <= x < 5 or x == 5 else 1 if x == 0 else x)

    return df