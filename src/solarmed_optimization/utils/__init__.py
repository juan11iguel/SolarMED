from dataclasses import asdict, fields
from enum import Enum
import json
import math
from collections.abc import Iterable
from typing import Any, Type, Literal
import time
from loguru import logger
import numpy as np
import pandas as pd
from solarmed_modeling.solar_med import SolarMED
from solarmed_modeling.fsms import MedState
from solarmed_modeling.fsms.med import FsmInputs as MedFsmInputs
from solarmed_modeling.fsms.sfts import FsmInputs as SfTsFsmInputs
from solarmed_optimization import (DecisionVariables, 
                                   DecisionVariablesUpdates, 
                                   EnvironmentVariables, 
                                   dump_at_index_dec_vars,
                                   ProblemSamples,
                                   ProblemParameters,
                                   RealLogicalDecVarDependence,
                                   RealDecVarsBoxBounds,
                                   MedMode,
                                   med_fsm_inputs_table,
                                   SfTsMode,
                                   sfts_fsm_inputs_table)
# from solarmed_optimization.problems import BaseMinlpProblem # circular import

def fitness_logger(func: callable) -> callable:
    def wrapper(*args, **kwargs) -> Any:
        print(f"Running function {func.__name__} with decision vector: {args[1]}")
        return func(*args, **kwargs)
    return wrapper

def timer_decorator(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()

        initial_states = kwargs.get('initial_states', None)
        max_step_idx = kwargs.get('max_step_idx', None)

        logger.info(f"Function {func.__name__} took {(end_time - start_time):.2f} seconds to run. Evaluated initial states {initial_states} for {max_step_idx} steps")
        return result
    return wrapper

def get_nested_attr(d: dict, attr: str) -> Any:
    """ Get nested attributes separated by a dot in a dictionary """
    keys = attr.split('.')
    for key in keys:
        d = d.get(key, None)
    return d

def flatten_list(nested_list: list[list]) -> list:
    if not isinstance(nested_list[0], Iterable):
        return nested_list # Not actually a nested list
    return [item for sublist in nested_list for item in sublist]
    

class CustomEncoder(json.JSONEncoder):
    """ Custom JSON encoder supporting NumPy arrays and Enums
        Example usage: json.dumps(array, cls=NumpyEncoder)
    """
    def default(self, obj):
        if isinstance(obj, np.ndarray):  # Handle NumPy arrays
            return obj.tolist()
        elif isinstance(obj, Enum):  # Handle Enums
            return obj.value
        return super().default(obj)
    
def get_valid_modes(fsm_inputs: MedFsmInputs, 
                    lookup_table: dict[tuple[MedMode, MedState], MedFsmInputs] = med_fsm_inputs_table,
                    return_values: bool = True) -> list[MedMode]:
    """ Return valid modes given a set of fsm inputs that can be applied to the system (generated by studying the FSM possible evolutions) """
    valid_modes = [
        key[0] for key, value in lookup_table.items() if value == fsm_inputs
    ]
    if not return_values:
        return valid_modes
    else:
        return [valid_mode.value for valid_mode in valid_modes]

    
def forward_fill_resample(source_array: np.ndarray, target_size: int, dtype: Type = None) -> np.ndarray:
    """Resample a smaller array to a larger array by forward filling

        Args:
            source_array (np.ndarray): Original array
            target_size (int): Size of the target array

        Returns:
            np.ndarray: Resampled array
            
        Example usage
            source_array = np.array([1, 2, 3])
            target_size = 10
            resampled_array = resample_array(source_array, target_size)
            assert len(resampled_array) == target_size, "Resampled array size mismatch"
    """
    if len(source_array) > target_size:
        logger.warning(f"Source array size {len(source_array)} is greater than to target size {target_size}. Returning trimmed source array")
        return source_array[:target_size]
    if len(source_array) == target_size:
        return source_array
    
    dtype: Type = type(source_array[0]) if dtype is None else dtype
    
    span = target_size // len(source_array)  # Integer division
    remainder = target_size % len(source_array)  # Remaining slots to fill
    
    # Repeat each element by span times
    resampled_array = np.repeat(source_array, span)
    
    # Forward fill the remaining slots if target_size isn't a multiple of source_array length
    if remainder > 0:
        resampled_array = np.concatenate((resampled_array, np.repeat(source_array[-1], remainder)))
    
    return resampled_array.astype(dtype)


def downsample_by_segments(source_array:  np.ndarray, target_size: int, dtype: Type = None) -> np.ndarray:
    """
    Downsamples the source array to the target size by selecting the mean value 
    in each segment of the array.

    Parameters:
        source_array (np.ndarray): The input array to be downsampled.
        target_size (int): The desired size of the output array.

    Returns:
        np.ndarray: The downsampled array, where each element is the mean value 
        of a segment from the input array.
    
    Raises:
        ValueError: If target_size is less than 1 or greater than the size of source_array.
        
    Example usage
        source_array = np.array([1, 1000, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9])
        target_size = 5
        downsampled_array = downsample_by_segments(source_array, target_size)

        print("Original array:", source_array)
        print("Downsampled array:", downsampled_array)
        assert len(downsampled_array) == target_size, "Downsampled array size mismatch"
    """
    if target_size < 1 or target_size > len(source_array):
        raise ValueError("target_size must be between 1 and the size of the source array.")

    dtype = dtype or source_array[0].dtype

    segment_size = len(source_array) / target_size
    return np.array([
        source_array[int(i * segment_size):int((i + 1) * segment_size)].mean()
        for i in range(target_size)
    ]).astype(dtype)

def resample_decision_variables(dec_vars: DecisionVariables, dec_var_updates: DecisionVariablesUpdates) -> DecisionVariables:
    """Resample decision variable object

    Args:
        dec_vars (DecisionVariables): _description_
        dec_var_updates (DecisionVariablesUpdates): _description_

    Returns:
        _type_: _description_
    """
    
    for var_id, var_values in asdict(dec_vars).items():
        target_size = getattr(dec_var_updates, var_id)
        if target_size >= len(var_values):
            setattr(dec_vars, var_id, forward_fill_resample(var_values, target_size))
        elif target_size < len(var_values):
            setattr(dec_vars, var_id, downsample_by_segments(var_values, target_size))
            
    return dec_vars
    

def decision_vector_to_decision_variables(x: np.ndarray, dec_var_updates: DecisionVariablesUpdates,
                                          span: Literal["optim_window", "optim_step", "none"],
                                          sample_time_mod: int = None, 
                                          optim_window_time: int = None,
                                          sample_time_opt: int = None) -> DecisionVariables:
    """ From decision vector x to DecisionVariables instance
        The decision vector is the one defined in the problem class:
        x = [ [1,...,n_updates_x1], [1,...,n_updates_x2], ..., [1,...,n_updates_xNdec_vars] ]
        where n_updates_xi is the number of updates for decision variable i along the prediction horizon
        
        The resulting instance contains a vector for every decision variable with
        a sample time equal to the one of the model.
    
        - When span is set to optim window, for every decision variable, all its
        updates are included in the resulting Decision Variables instance and the
        number of elements is equal to the number of model evaluations along the 
        optimization window.
        
        - When span is set to optim step, for every decision variable, only the updates
        contained within one optimization step are included in the resulting instance, 
        and the number of elements is equal to the number of model evaluations along 
        one optimization step.
    """
    
    assert span in ["optim_window", "optim_step", "none"], "span should be either 'optim_window' or 'optim_step'"
    if span in ["optim_window", "optim_step"]:
        assert sample_time_mod is not None, "If span is 'optim_window' or 'optim_step', sample_time_mod should be provided"
        assert optim_window_time is not None, "If span is 'optim_window', optim_window_time should be provided"
    if span == "optim_step":
        assert sample_time_opt is not None, "If span is 'optim_step', sample_time_opt should be provided"

    n_evals_mod = None    
    if span == "optim_step":
        # As many model evaluations as samples that fit in the optimization step 
        n_evals_mod = math.floor(sample_time_opt / sample_time_mod)
    elif span == "optim_window":
        # As many model evaluations as samples that fit in the optimization window
        n_evals_mod = math.floor(optim_window_time / sample_time_mod)
        
    # Build the decision variables dictionary in which every variable is "resampled" to the model sample time
    decision_dict: dict[str, np.ndarray] = {}
    cnt = 0
    # for var_id, num_updates_optim_window in asdict(dec_var_updates).items():
    # dec_var_updates does not necessary follow the same order as DecisionVariables
    for fld in fields(DecisionVariables):
        var_id = fld.name
        num_updates_optim_window = getattr(dec_var_updates, var_id)
        
        if span == "optim_step":
            # Only the updates in the optimization step are considered
            # |o----o----o----o--| : 4 updates in the optimization window (size = optim_window_time)
            # |________| : 2 updates in the optimization step (size = sample_time_opt)
            num_updates = math.floor(num_updates_optim_window * sample_time_opt / optim_window_time)
            # print(f"{num_updates=}")
        else:
            # All the updates in the optimization window are considered
            num_updates = num_updates_optim_window

        if n_evals_mod is not None:            
            decision_dict[var_id] = forward_fill_resample(x[cnt:cnt+num_updates], target_size=n_evals_mod)
        else:
            decision_dict[var_id] = x[cnt:cnt+num_updates]
        cnt += num_updates_optim_window
    
    return DecisionVariables(**decision_dict)

def decision_variables_to_decision_vector(dec_vars: DecisionVariables, dec_var_updates: DecisionVariablesUpdates = None) -> np.ndarray:
    """ From DecisionVariables instance to decision vector x
        The decision vector is the one defined in the problem class:
        x = [ [1,...,n_updates_x1], [1,...,n_updates_x2], ..., [1,...,n_updates_xNdec_vars] ]
        where n_updates_xi is the number of updates for decision variable i along the prediction horizon
    """
    if dec_var_updates is not None:
        dec_vars = resample_decision_variables(dec_vars, dec_var_updates)
                
    return np.concatenate([var_values for var_values in asdict(dec_vars).values()])
        
def compute_dec_var_differences(dec_vars: dict[str, float], model_dec_vars: dict[str, float], model_dec_var_ids: list[str]) -> np.ndarray[float]:
    """ This implementation compared to initializing two dataframes is much faster:
    # Results
    {
        'list_comprehension_time': 0.00038123130798339844,
        'dataframe_time': 0.0037851333618164062,
    }
    """
    # Align both dictionaries' values using the common order of model_dec_var_ids
    # ordered_dv_values = [dv_dict[key] for key in model_dec_var_ids] # Already ordered
    dec_vars_values: list[float] = list(dec_vars.values())
    ordered_model_values: list[float] = [model_dec_vars[key] for key in model_dec_var_ids]

    # Compute absolute differences
    return np.abs(
        np.array(dec_vars_values, dtype=float) - np.array(ordered_model_values, dtype=float)
    )
    
def validate_dec_var_updates(dec_var_updates: DecisionVariablesUpdates, optim_window_time: int, sample_time_mod: int) -> None:
    """ 
    Validate that the number of updates for each decision variable is within the bounds
    of the optimization window or optimization step.
    """
    n_evals_mod_in_hor_window: int = math.floor(optim_window_time  / sample_time_mod)
    max_dec_var_updates: int = n_evals_mod_in_hor_window
    min_dec_var_updates: int = 1

    for var_id, value in asdict(dec_var_updates).items():
        try:
            assert value <= max_dec_var_updates and value >= min_dec_var_updates, \
            f"Invalid number of updates for variable {var_id}, n updates = {value}, should be {min_dec_var_updates} <= n updates <= {max_dec_var_updates}"
        except AssertionError as e:
            logger.warning(f"{e}. Setting value to bound")
            
            if value < min_dec_var_updates:
                setattr(dec_var_updates, var_id, min_dec_var_updates)
            else:
                setattr(dec_var_updates, var_id, max_dec_var_updates)
            

def evaluate_model(model: SolarMED, 
                   dec_vars: DecisionVariables, 
                   env_vars: EnvironmentVariables,
                   n_evals_mod: int,
                   mode: Literal["optimization", "evaluation"] = "optimization",
                #    model_dec_var_ids: list[str] = None,
                   df_mod: pd.DataFrame = None,
                   df_start_idx: int = None) -> pd.DataFrame | float:
    """ Evaluate the model for a given decision vector and environment variables
        n_evals_mod is the number of model evaluations, whose value depends on what
        is being evaluated:
        - If mode is optimization, n_evals_mod should be the number of model evaluations in the optimization window 
        (optim_window_time // sample_time_mod)
        - If mode is evaluation, n_evals_mod should be the number of model evaluations in one optimization step 
        (sample_time_opt // sample_time_mod)
        - Though an arbitrary number of evaluations can be performed, make sure that `n_evals_mod` is lower or equal 
        to the number of elements in the decision vector and environment variables.
    """
    # if mode == "optimization":
    #     assert model_dec_var_ids is not None, "`model_dec_var_ids` is required if `mode` is set to 'optimization'"
    
    # if df_mod is None and mode == "evaluation":
    #     df_mod = model.to_dataframe()
        
    if mode == "optimization":
        fitness: np.ndarray[float] = np.zeros((n_evals_mod, ))
        ics: np.ndarray[float] = np.zeros((n_evals_mod, len(model_dec_var_ids)))
    
    # dec_var_ids = list(asdict(dec_vars).keys())
    for step_idx in range(n_evals_mod):
        dv: DecisionVariables = dump_at_index_dec_vars(dec_vars, step_idx)
        
        # print(f"{dv.med_vac_state=}")
        
        # Get the MED FSM inputs for the current MED mode and state
        med_fsm_inputs: MedFsmInputs = med_fsm_inputs_table[ (MedMode(dv.med_mode), model.med_state) ]
        sfts_fsm_inputs: SfTsFsmInputs = sfts_fsm_inputs_table[ (SfTsMode(dv.sfts_mode), model.sf_ts_state) ]
        
        # TODO: Add here some low-level control/validation
        # - qts_src should be zero if Tsf_out is below Tts_c_b? Tts_h_t? Which temperature should be the threshold?
        
        model.step(
            # Decision variables
            ## Thermal storage
            qts_src = dv.qts_src * sfts_fsm_inputs.ts_active,
            
            ## Solar field
            qsf = dv.qsf * sfts_fsm_inputs.sf_active,
            
            ## MED
            qmed_s = dv.qmed_s * med_fsm_inputs.med_active,
            qmed_f = dv.qmed_f * med_fsm_inputs.med_active,
            Tmed_s_in = dv.Tmed_s_in,
            Tmed_c_out = dv.Tmed_c_out,
            med_vacuum_state = med_fsm_inputs.med_vacuum_state,
            
            ## Environment
            I=env_vars.I[step_idx],
            Tamb=env_vars.Tamb[step_idx],
            Tmed_c_in=env_vars.Tmed_c_in[step_idx],
            wmed_f=env_vars.wmed_f[step_idx] if env_vars.wmed_f is not None else None,
            
            # Additional parameters
            compute_fitness=True if mode == "evaluation" else False
        )
        
        if mode == "optimization":
            # Inequality contraints, decision variables should be the same after model evaluation: |dec_vars-dec_vars_model| < tol
            ics[step_idx, :] = None
            # TODO: Fix this after change of Med decision variable to an indirect one
            # ics[step_idx, :] = compute_dec_var_differences(dec_vars=asdict(dv), 
            #                                                model_dec_vars=model.model_dump(include=model_dec_var_ids),
            #                                                model_dec_var_ids=model_dec_var_ids)
            fitness[step_idx] = model.evaluate_fitness_function(
                cost_e=env_vars.cost_e[step_idx],
                cost_w=env_vars.cost_w[step_idx],
                objective_type='minimize'
            )
        if mode == "evaluation":
            df_mod = model.to_dataframe(df_mod, )
                
    if mode == "optimization":
        return np.sum(fitness), ics.mean(axis=0)
    elif mode == "evaluation":
        if df_start_idx is not None:
            df_mod.index = pd.RangeIndex(start=df_start_idx, stop=len(df_mod)+df_start_idx)
        return df_mod#, ics # ic temporary to validate
    else:
        raise ValueError(f"Invalid mode: {mode}")
    
    
def add_bounds_to_dataframe(df: pd.DataFrame, problem, target: Literal['optim_step', 'optim_window'], df_idx: int = 0) -> pd.DataFrame:
                            #target_size: int, source_size: int = None, 
    
    for var_idx, var_id in enumerate(problem.dec_var_ids):
        if f"lower_bounds_{var_id}" not in df.columns:
            df[f"upper_bounds_{var_id}"] = np.nan
            df[f"lower_bounds_{var_id}"] = np.nan
                
        if target == "optim_step":
            source_size = problem.n_updates_per_opt_step[var_idx]
            target_size = problem.n_evals_mod_in_opt_step
        else:
            source_size = getattr(problem.dec_var_updates, var_id)
            target_size = problem.n_evals_mod_in_hor_window
                
        if var_id in problem.dec_var_int_ids:
            discrete_bounds = problem.integer_dec_vars_mapping[var_id][:source_size]
            upper_bounds_eval = [np.max(db) for db in discrete_bounds]
            lower_bounds_eval = [np.min(db) for db in discrete_bounds]
        else:
            upper_bounds_eval = problem.box_bounds_upper[var_idx][:source_size]
            lower_bounds_eval = problem.box_bounds_lower[var_idx][:source_size]
        
        lower_bounds_eval = forward_fill_resample(lower_bounds_eval, target_size=target_size)
        upper_bounds_eval = forward_fill_resample(upper_bounds_eval, target_size=target_size)
        
        df.loc[df_idx:df_idx+target_size, f"upper_bounds_{var_id}"] = upper_bounds_eval
        df.loc[df_idx:df_idx+target_size, f"lower_bounds_{var_id}"] = lower_bounds_eval
        
    return df

def add_dec_vars_to_dataframe(df: pd.DataFrame, dec_vars: DecisionVariables, df_idx: int = 0) -> pd.DataFrame:

    for var_id, var_values in asdict(dec_vars).items():
        df.loc[df_idx:df_idx+len(var_values), f"dec_var_{var_id}"] = var_values
        
    return df

def times_to_samples(pp: ProblemParameters) -> ProblemSamples:
    # Times to samples transformation
    
    assert pp.episode_duration is not None, "Episode duration must be defined before calling this function"
    
    n_evals_mod_in_hor_window = math.floor(pp.optim_window_time  / pp.sample_time_mod)
    max_dec_var_updates = n_evals_mod_in_hor_window
    episode_samples = pp.episode_duration // pp.sample_time_mod
    optim_window_samples = pp.optim_window_time // pp.sample_time_mod
    n_evals_mod_in_opt_step = math.floor(pp.sample_time_opt / pp.sample_time_mod)
    span = math.ceil(pp.fixed_model_params.sf.delay_span / pp.sample_time_mod)
    idx_start = span if pp.idx_start is None else pp.idx_start
    
    return ProblemSamples(
        n_evals_mod_in_hor_window = n_evals_mod_in_hor_window,
        n_evals_mod_in_opt_step = n_evals_mod_in_opt_step,
        max_dec_var_updates = max_dec_var_updates,
        episode_samples = episode_samples,
        optim_window_samples = optim_window_samples,
        max_opt_steps = (episode_samples-idx_start-optim_window_samples) // n_evals_mod_in_opt_step - 1,
        default_n_dec_var_updates = min(math.floor(pp.optim_window_time / pp.sample_time_opt), max_dec_var_updates),
        span=span
    )

def validate_real_dec_vars(dec_vars: DecisionVariables, real_dec_var_bounds: RealDecVarsBoxBounds) -> DecisionVariables:
    """
    Validate real decision variables values. 
    - If a value is below the lower limit, set it to the lower limit
    - If a value is above the upper limit, set it to the upper limit
    """
    for var_id, bounds in asdict(real_dec_var_bounds).items():
        lower_limit, upper_limit = bounds
        var_values = getattr(dec_vars, var_id)
        integer_dec_var_val = forward_fill_resample(getattr( dec_vars, RealLogicalDecVarDependence[var_id].value), 
                                                    target_size=var_values.shape[0])
        
        var_values = np.clip(var_values, lower_limit, upper_limit)
        var_values = var_values * integer_dec_var_val
        
        setattr(dec_vars, var_id, var_values)
    
    return dec_vars