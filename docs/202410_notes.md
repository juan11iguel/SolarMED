

- New library: [pygmo](https://esa.github.io/pygmo2)

## Problem definition

![](attachments/solarMED_optimization-Decision%20tree.svg)

### Decision variables


**Logical / integers**

- `sf_state`: Solar field state (off, active)
- `ts_state`: Thermal storage state (off, active)
- `med_vac_state`: MED vacuum system state (off, active)
- `med_s_state`: MED heat source state (off, active)

**Real**

- `qsf`: Solar field flow -> Actual optimization output will be the outlet temperature (`Tsf,out`) after evaluating the inverse solar field model.
- `qts_src`: Thermal storage recharge flow.
- `qmed_s`: MED heat source flow.
- `qmed_f`: MED feed flow.
- `Tmed_s_in`: MED heat source inlet temperature.
- `Tmed_c_out`: MED condenser outlet temperature.

#### Decision vector structure

The decision vector will be composed by batches of decision variables ($N_{dec,vars}$) for each step in the horizon ($N_{steps}$):

$\mathbb R^{n_{dec,vars,r}}
\times  \mathbb Z^{n_{dec,vars,i}} \rightarrow \mathbb R^{n_{dec,vars}}$

$N = \mathbb R^{n_{dec,vars}} \times N_{steps}$

$\textrm{dec\_vec} = [\textrm{sf}_{st}(1), \textrm{ts}_{st}(1), \textrm{med}_{vac,state}(1), \textrm{med}_{s,st}(1), ..., \textrm{sf}_{st}(N), \textrm{ts}_{st}(N), \textrm{med}_{vac,state}(N), \textrm{med}_{s,st}(N)]$

### Environment variables

- `Tamb`
- `I`
- `Tmed_c_in`


### Costs variables

- `cost_w`: Cost of water
- `cost_e`: Cost of electricity

## Considerations

Artificially modifying the problem with unnecessary restrictions or by modifying the problem itself should be avoided as much as possible. Two examples:

- Trying to set as objective maximizing the operation time. If for example the optimization algorithm in one iteration tries values that invalidate the system midway through the evaluation and does not recover, the model automatically is going to penalize this behavior with a worse cost function value. And so eventually the most fit individuals or solutions will be the ones that prolong the operation for **as long as it makes *economical* sense** for the given horizon. 
- Restricting the state changes. The model itself will penalize constant state changes since it has associated transient non-productive states (*generating vacuum, starting-up, shutting-down*) between steady states (*off, idle, active*). This will result in a worse cost obtained at the end of the path evaluation.

## Resolution strategies

One of the most important considerations when attempting to solve the optimization problem, is whether the paths to take (operating modes evolution in the prediction horizon) are pre-computed or if on the other hand, it's left to the optimization algorithm to explore the tree of possible paths and generate its own trajectories.

### Complete problem (MINLP)

#### Random initialization of all variables

#### Random initialization of real variables + heuristic initialization of integer part

Look for the function that pygmo uses to initialize the population [`pygmo.random_decision_vector(prob)`](https://esa.github.io/pygmo2/generic_utils.html#pygmo.random_decision_vector), and modify it to initialize the integer part of the decision vector with the heuristic.

We make use of `path_explorer` for, given the initial state of the subsystem, get all the possible paths after all the steps are evaluated. Each state is assigned some value and we select the `n_pop` best paths to be evaluated.


### 2 problems: few simpler ILP problems followed by NLP problems.

First, solve the complete problem fixing the real variables to its low bounds, upper bounds, and/or some values in between. Then, solve the NLP problems for each integer alternative using a gradient and some NLP solver.

### N NLP problems

Explore all possible paths given an initial state using `path_explorer`, with a lower sample rate (~1h) and select N of them.

Depending on the problems we can solve in parallel, and the time it takes the individual problems to converge, we could some Np problems in parallel, and Ns sequentially. So N = Np * Ns problems in total.

Solve the selected ones as a NLP problem (selection could be as in [heuristic initialization](#random-initialization-of-real-variables--heuristic-initialization-of-integer-part)) implementing a gradient and using some NLP solver. Here we use the normal sample time (15 / 30 min). The integer variables are fixed and ffilled.

## Algorithms to test

#TODO

## Implementation details

### `fitness` method

The fitness function should simply evaluate the model with the given decision variables, the only additional task would be to calculate the constraints violation by taking the difference between some of the decision variables and the ones validated by the model, and booleans indicating if some operational limits are violated (e.g. solar field outlet temperature is above max., same for thermal storage).

### `get_bounds` method

Here, provided an initial state for each subsystem, we can call `path_explorer` to somehow determine the valid logical (integer) input values.

## Implementation timeline / steps

1. Add support for cooldown times in FSMs.
3. Check/modify `path_explorer` to return the best N paths given an initial state.
5. Implement [complete MINLP problem](#complete-problem-minlp) resolution strategy
   1. Implement `get_bounds` method.
   2. Implement `fitness` method.
6. Implement [N NLP problems](#n-nlp-problems) resolution strategy
   1. Implement `get_bounds` method.
   2. Implement `fitness` method.
7. For each candidate resolution strategy, test it with different algorithms using pygmo's island/archipielagos for one iteration. Save results (df predicciones de dec_vars, environment vars, etc, stats population, algorithm, etc).
8. Visualize and analyze results: algorithm convergence, best, solution, etc. Compare algorithms/strategies in terms of fitness vs n fitness evaluations / computational time.
9. Choose the best algorithm for each resolution strategy.
10. Evaluate the strategies for one episode  (several iterations) using the chosen algorithms.
11. Visualize animation of timeseries evolution with actual and predicted values in each iteration.  

Fine tune chosen algorithm parameters using a tool like [Optuna](https://github.com/optuna/optuna).